Why use Docker? Docker makes it really easy to install and run software without worrying about setup or dependencies.

What is Docker? Docker is a platform or ecosystem around creating and running containers. Docker can be referring to Docker Client, Docker Server, Docker Machine, Docker Images, Docker Hub, or Docker Compose.

Image - Single file with all the deps and config required to run a program.
Container - Instance of an image. Runs a program with it's own isolated set of hardware resources. 

Docker Client - Tool taht we are going to issue commands to.
Docker Server (Docker Daemon) - Tool that is responsible for creating images, running containers, etc

If you use ufw, be aware that when you expose container ports using docker, these ports bypass your firewall rules. For more info, check https://docs.docker.com/engine/network/packet-filtering-firewalls/#docker-and-ufw. 

Installing Docker depends on environment. There are some restrictions such has Docker Desktop cannot be installed on OSes in a VM because 'Docker Desktop does not work with nested virtualization'.  

Docker Desktop on Linux runs a Virtual Machine (VM) which creates and uses a custom docker context, 'desktop-linux', on startup. 

Docker Desktop for Linux provides a user-friendly graphical interface that simplifies the management of containers and services. It includes Docker Engine as this is the core technology that powers Docker containers. Docker Desktop for Linux also comes with additional features like Docker Scout and Docker Extensions.

Installing Docker Desktop and Docker Engine
Docker Desktop for Linux and Docker Engine can be installed side-by-side on the same machine. Docker Desktop for Linux stores containers and images in an isolated storage location within a VM and offers controls to restrict its resources. Using a dedicated storage location for Docker Desktop prevents it from interfering with a Docker Engine installation on the same machine.

While it's possible to run both Docker Desktop and Docker Engine simultaneously, there may be situations where running both at the same time can cause issues. For example, when mapping network ports (-p / --publish) for containers, both Docker Desktop and Docker Engine may attempt to reserve the same port on your machine, which can lead to conflicts ("port already in use").

We generally recommend stopping the Docker Engine while you're using Docker Desktop to prevent the Docker Engine from consuming resources and to prevent conflicts as described above.

Use the following command to stop the Docker Engine service:
```bash
$ sudo systemctl stop docker docker.socket containerd
```
Depending on your installation, the Docker Engine may be configured to automatically start as a system service when your machine starts. Use the following command to disable the Docker Engine service, and to prevent it from starting automatically:
```bash
$ sudo systemctl disable docker docker.socket containerd
```

Switching between Docker Desktop and Docker Engine
The Docker CLI can be used to interact with multiple Docker Engines. For example, you can use the same Docker CLI to control a local Docker Engine and to control a remote Docker Engine instance running in the cloud. Docker Contexts allow you to switch between Docker Engines instances.

When installing Docker Desktop, a dedicated "desktop-linux" context is created to interact with Docker Desktop. On startup, Docker Desktop automatically sets its own context (desktop-linux) as the current context. This means that subsequent Docker CLI commands target Docker Desktop. On shutdown, Docker Desktop resets the current context to the default context.

Use the docker context ls command to view what contexts are available on your machine. The current context is indicated with an asterisk (*).
```bash
$ docker context ls
NAME            DESCRIPTION                               DOCKER ENDPOINT                                  ...
default *       Current DOCKER_HOST based configuration   unix:///var/run/docker.sock                      ...
desktop-linux                                             unix:///home/<user>/.docker/desktop/docker.sock  ...        
```
If you have both Docker Desktop and Docker Engine installed on the same machine, you can run the docker context use command to switch between the Docker Desktop and Docker Engine contexts. For example, use the "default" context to interact with the Docker Engine:
```bash
$ docker context use default
default
Current context is now "default"
```

And use the desktop-linux context to interact with Docker Desktop:
```bash
$ docker context use desktop-linux
desktop-linux
Current context is now "desktop-linux"
Refer to the Docker Context documentation for more details.
```


## Manage Docker as a non-root user
The Docker daemon binds to a Unix socket, not a TCP port. By default it's the root user that owns the Unix socket, and other users can only access it using sudo. The Docker daemon always runs as the root user. If you don't want to preface the docker command with sudo, create a Unix group called docker and add users to it. When the Docker daemon starts, it creates a Unix socket accessible by members of the docker group. On some Linux distributions, the system automatically creates this group when installing Docker Engine using a package manager. In that case, there is no need for you to manually create the group.  (The docker group grants root-level privileges to the user. To run Docker without root privileges, see look online to run the Docker daemon as a non-root user (Rootless mode).)

## Docker logs
Docker provides logging drivers for collecting and viewing log data from all containers running on a host. The default logging driver, json-file, writes log data to JSON-formatted files on the host filesystem. Over time, these log files expand in size, leading to potential exhaustion of disk resources. (See: https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user)



Image Cache - where downloaded images are stored

When running docker run <image>, the very first time it will download the image from docker hub, then store it in the image cache. on subsequent 'docker run <image>' commands, docker will use the image cache and not have to download from docker hub.

docker containers are possible behind the scenes using 'namespacing' and 'cgroups'. 
namespacing - isolating resources per process (or group of processes)
control groups (cgroups) - limit AMOUNT of resources used per process - memory, cpu usage, hd i/o, network bandwith

An image is a file system snapshot + a startup command. That file system is put into containers.

A container is an isolated running process with its own isolated share of hardware resources. 

On mac and windows, docker is running within a linux VM, because of the need for namespacing and cgroups as features. So docker is always running in a linux OS environment.

## Commands

To create and start a container:
- docker run <image-name>
Example:
docker run hello-world
use -d to run container in the background

To override default command:
- docker run <image-name> <command>
Example:
docker run busybox ls
docker run busybox echo "hello world"
above commands don't work with hello-world image because ls and echo executable files are not in the file system snapshot of the hello-world image.

To list all running containers;
- docker ps
- docker ps --all // for all containers that that we ever started up

To create a container:
- docker create <image-name>
Places image's file system snapshot into hard drive segment for container

To start a container:
- docker start -a <container-id>
Executing the startup command in a created container
The -a attaches the container's output to your terminal

To remove all stopped (exited) containers, and image build cache:
- docker system prune

To get logs from a container:
- docker logs <container-id>
Get history of stdout

To send SIGTERM signal to docker process:
- docker stop <container-id>
Give container 10 seconds to perform cleanup

To send SIGKILL signal to docker process:
- docker kill <container-id>
Shuts down immediately

To execute an additional command in a container:
- docker exec -it <container-id> <command>
-i flag redirects our terminal to the stdin of the command
-t makes sure text from stdin and stdout is formatted properly

To open a shell into container:
- docker exec -it <container-id> sh
CTRL+C, CTRL+D or type 'exit' to exit

To create and start a container with opened shell:
- docker run -it <image-name> sh
Will not run any default command

To build an image from a Dockerfile
- docker build .
- docker build -t <docker-username>/<project-name>:<tag>
The . represents the build context, the place where the build files (Dockerfile) are
Use -t to name the image

To manually create an image from a container
- docker commit -c <default-command> <container-id>
Example: docker commit -c 'CMD ["redis-server"]' 39075252a24242s

To tag (name) an image after its been created:
- docker tag <container-id> <tag>
- docker tag 9242020d242 realmohsin67/redis:1.0.0

To run with port mapping:
- docker run -p <host-port>:<container-port> <image-name>

To run containers specified in docker-compose.yml
- docker compose up
Similar to docker run <image-name>
use -d to launch all containers in background

To build images then run containers specified in docker-compose.yml
- docker compose up --build
Similar to `docker build . && docker run <image-name>`

To stop all containers from a docker-compose.yml file at once
- docker compose down

Once you have a created container, you cannot replace the default command. If you start a container that has exited (stopped), the default command will be rerun.

Every process we create in a linux environment has 3 communication channels atached to it that we refer to as - sdout, stdin and stderr. These channels are used to communicate information either into the process or out of the process. 


# Building Custom Images Through Docker Server

To create your own image you have to create a Dockerfile. The Dockerfile will be given to the docker server by teh docker client and a usable image will be created.
```Dockerfile
FROM alpine
RUN apk add --update redis
CMD ["redis-server"]
```
FROM is used to specify base image, which is downloaded, then a temporary container is started from base image and the RUN command is executed, and the resulting updated file system is snapshotted as a new image.

NOTES: FROM copies the filesytem and default command of the base image into the custom image being created.

At every step of a Dockerfile, an image is created. The image created by the final step is the final image, but every intermediate step runs a temporary container to create an intermediate image. These intermediate images are saved in a cache to be reused during future builds. So if you change your Dockerfile the image does not have to be rebuilt from scratch everytime. 

## Image Tag
Naming an image can be called 'tagging an image', but at the same time, the 'tag' of an image is just what's after the ':' in the conventionally naming pattern. So tag is the version of an image, but can mean other thing as well?


## Manually create an Image from a container
You can manually build an image from running containers. For example, to manually build the image the above Dockerfile builds, you can do the following:
```bash
docker run -it alpine sh
apk add --update redis

# new terminal window
docker ps # to get the container id of the running alpine container
docker commit -c 'CMD ["redis-server"]' 3095724242a24 # create image with given default command
```

NOTE: When you don't specify a tag for an image, the 'latest' tag is used. This can cause problems later when the version of the image changes.

alpine versions of images are the most lightweight, with just the necessary packages installed. Example node:14-alpine just has node, npm and other basic packages only

## To create image using local files
```Dockerfile
FROM node:14-alpine
WORKDIR /usr/app
COPY ./ ./
RUN npm install
CMD ["redis-server"]
```
By default, no traffic that is coming into your computer or into your localhost network is routed into the container. The container has its own isolated set of ports that can receive traffic. To have traffic coming into the computer routed to the container, we need to setup an explicit port mapping. This is only for incoming requests. By default, containers can make outgoing requests on their own.

WORKDIR not only affects subsequent commands in the Dockerfile, but also affects 'docker exec' commands run manually.

When making a change to files that you want to copy over, when rebuilding image, that step AND EVERY STEP AFTER IT will have to be redone without using cache. To make sure npm install does not trigger again unnecessarily, and makes use of cached intermediate image, you can do the following:
```Dockerfile   
FROM node:14-alpine
WORKDIR /usr/app
COPY ./package-json ./
RUN npm install
COPY ./ ./
CMD ["redis-server"]
```

## Docker Compose
docker compose is a separate CLI that gets installed along with Docker. It's used to start up multiple docker containers at the same time. Automates some of hte long-winded arguments we were passing to 'docker run'.

docker-compose.yml is used to specify commands.
```yaml
version: '3'
services:
  redis-server:
    image: 'redis'
  node-app:
    build: .
    ports:
      - "4001:8081"
```
Just by defining containers in docker-compose.yml the containers will be set up to be on the same network. They can communicate with each other without having to set up ports between the two.

if process exits with code 0, it signifies an intentional exit. Any other code signifies something went wrong.